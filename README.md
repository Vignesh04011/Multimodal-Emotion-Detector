This is my 3rd year (6th semester) project, where I built a Multimodal Emotion Detection App. The app uses three types of data to predict human emotions in real-time:

1. Facial expressions (from camera)

2. Voice features (MFCC)

3. Physiological data (BPM, SPO2, temperature)

🔧 What I Built
1. Collected real-time data from sensors, mic, and camera

2. Preprocessed the data separately for all three inputs

3. Trained individual ML models for each type of data

4. Combined the outputs using a fusion model

5. Built a live dashboard using React.js and Firebase to display emotions and sensor values

⚠️ Important Note: Code Not Available
The full project was successfully completed, but due to a sudden system crash, I had to format my laptop. At that time, I had not pushed the project code to GitHub, so the entire source code was lost.

I understand this is a major issue, and I’ve now learned the importance of regular backups and version control.

📑 What I Still Have
Even though the code is lost, I have a detailed project report that includes:

1. Project architecture and workflow

3. ML model designs and accuracy results

3. Data preprocessing steps

4. Dashboard UI and functionality

5. Challenges I faced and how I solved them

📌 I can share the report and explain the full implementation if needed.

🧠 What I Learned
1. Handling and combining multiple data types (images, audio, sensor data)

2. Training and fusing ML models

3 .Creating a responsive live dashboard

4. Firebase integration with React.js

5. And most importantly: always use GitHub from day one!
