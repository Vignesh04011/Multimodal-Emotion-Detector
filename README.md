This is my 3rd year (6th semester) project, where I built a Multimodal Emotion Detection App. The app uses three types of data to predict human emotions in real-time:

Facial expressions (from camera)

Voice features (MFCC)

Physiological data (BPM, SPO2, temperature)

🔧 What I Built
Collected real-time data from sensors, mic, and camera

Preprocessed the data separately for all three inputs

Trained individual ML models for each type of data

Combined the outputs using a fusion model

Built a live dashboard using React.js and Firebase to display emotions and sensor values

⚠️ Important Note: Code Not Available
The full project was successfully completed, but due to a sudden system crash, I had to format my laptop. At that time, I had not pushed the project code to GitHub, so the entire source code was lost.

I understand this is a major issue, and I’ve now learned the importance of regular backups and version control.

📑 What I Still Have
Even though the code is lost, I have a detailed project report that includes:

Project architecture and workflow

ML model designs and accuracy results

Data preprocessing steps

Dashboard UI and functionality

Challenges I faced and how I solved them

📌 I can share the report and explain the full implementation if needed.

🧠 What I Learned
Handling and combining multiple data types (images, audio, sensor data)

Training and fusing ML models

Creating a responsive live dashboard

Firebase integration with React.js

And most importantly: always use GitHub from day one!
