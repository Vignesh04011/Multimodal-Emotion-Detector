# Multimodal-Emotion-Detector
This is my 3rd year even semester project(VI semester) where i have created a project in which 3 different parameters(Facial, Camera, Physiology) is trained and is used to predict the emotion
I successfully completed a Multimodal Emotion Detector App that used facial expression recognition, voice analysis (MFCC features), and physiological data (like BPM, SPO2, and temperature) to predict real-time emotions. The project involved a full pipelineâ€”data collection, preprocessing, model training (individual and fusion models), and live visualization via a custom dashboard using React.js and Firebase.

Unfortunately, a critical system failure required me to format my laptop, and I had not yet pushed the code to GitHub at that time. As a result, the source code was lost.

However, I have a detailed project report documenting:

Project architecture

Model design and accuracy

Data processing pipelines

Challenges and solutions

Dashboard interface and functionalities
